import pytest
import torch

from circuit_tracer.replacement_model import ReplacementModel
from circuit_tracer.replacement_model.replacement_model_nnsight import NNSightReplacementModel
from circuit_tracer.replacement_model.replacement_model_transformerlens import (
    TransformerLensReplacementModel,
)


class TestGetActivations:
    """Test get_activations method for both nnsight and transformer_lens implementations."""

    @pytest.fixture(scope="class")
    def models(self) -> tuple[NNSightReplacementModel, TransformerLensReplacementModel]:
        """Load both nnsight and transformer_lens ReplacementModels."""
        # Use preset "gemma" transcoder set which should be identical for both
        model_name = "google/gemma-2-2b"

        # Load nnsight model
        nnsight_model = ReplacementModel.from_pretrained(
            model_name,
            "gemma",  # Use preset name
            device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
            dtype=torch.bfloat16,
            backend="nnsight",
        )

        # Load transformer_lens model
        tl_model = ReplacementModel.from_pretrained(
            model_name,
            "gemma",  # Use same preset name
            device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
            dtype=torch.bfloat16,
        )

        assert isinstance(nnsight_model, NNSightReplacementModel)
        assert isinstance(tl_model, TransformerLensReplacementModel)
        return nnsight_model, tl_model

    @pytest.fixture
    def test_inputs(self):
        """Provide test inputs for activation extraction."""
        return [
            "Hello world",
            "The quick brown fox jumps over the lazy dog",
            "This is a test sentence for the activation comparison.",
            "Apple is a company that makes iPhones and computers",
        ]

    def test_get_activations_shape_consistency(self, models, test_inputs):
        """Test that both models return activations with the same shape."""
        nnsight_model, tl_model = models

        for input_text in test_inputs:
            # Get activations from both models
            nnsight_logits, nnsight_activations = nnsight_model.get_activations(input_text)
            tl_logits, tl_activations = tl_model.get_activations(input_text)

            # Check logits shape
            assert nnsight_logits.shape == tl_logits.shape, (
                f"Logits shape mismatch for input '{input_text}': "
                f"nnsight {nnsight_logits.shape} vs TL {tl_logits.shape}"
            )

            # Check activations shape
            assert nnsight_activations.shape == tl_activations.shape, (
                f"Activations shape mismatch for input '{input_text}': "
                f"nnsight {nnsight_activations.shape} vs TL {tl_activations.shape}"
            )

    def test_get_activations_values_close(self, models, test_inputs):
        """Test that both models return similar activation values."""
        nnsight_model, tl_model = models

        for input_text in test_inputs:
            # Get activations from both models
            nnsight_logits, nnsight_activations = nnsight_model.get_activations(input_text)
            tl_logits, tl_activations = tl_model.get_activations(input_text)

            # Check that logits are close
            assert torch.allclose(nnsight_logits, tl_logits, atol=1e-3, rtol=1e-3), (
                f"Logits not close for input '{input_text}': "
                f"max diff = {torch.max(torch.abs(nnsight_logits - tl_logits))}"
            )

            # Check that activations are close (allowing for some numerical differences)
            assert torch.allclose(nnsight_activations, tl_activations, atol=1e-3, rtol=1e-3), (
                f"Activations not close for input '{input_text}': "
                f"max diff = {torch.max(torch.abs(nnsight_activations - tl_activations))}"
            )

    def test_get_activations_sparse_option(self, models, test_inputs):
        """Test sparse activation option for both models."""
        nnsight_model, tl_model = models
        input_text = test_inputs[0]  # Use first test input

        # Test sparse=True
        nnsight_logits_sparse, nnsight_activations_sparse = nnsight_model.get_activations(
            input_text, sparse=True
        )
        tl_logits_sparse, tl_activations_sparse = tl_model.get_activations(input_text, sparse=True)

        # Check that sparse tensors are actually sparse
        assert nnsight_activations_sparse.is_sparse, "nnsight sparse activations should be sparse"
        assert tl_activations_sparse.is_sparse, "TL sparse activations should be sparse"

        # Compare with dense versions
        nnsight_logits_dense, nnsight_activations_dense = nnsight_model.get_activations(
            input_text, sparse=False
        )
        tl_logits_dense, tl_activations_dense = tl_model.get_activations(input_text, sparse=False)

        # Logits should be the same regardless of sparse option
        assert torch.allclose(nnsight_logits_sparse, nnsight_logits_dense, atol=1e-6)
        assert torch.allclose(tl_logits_sparse, tl_logits_dense, atol=1e-6)

        # Dense versions of sparse activations should match dense activations
        assert torch.allclose(
            nnsight_activations_sparse.to_dense(), nnsight_activations_dense, atol=1e-6
        )
        assert torch.allclose(tl_activations_sparse.to_dense(), tl_activations_dense, atol=1e-6)

    def test_get_activations_zero_bos_option(self, models, test_inputs):
        """Test zero_bos option for both models."""
        nnsight_model, tl_model = models
        input_text = test_inputs[0]  # Use first test input

        # Test zero_bos=True
        nnsight_logits_zero, nnsight_activations_zero = nnsight_model.get_activations(
            input_text, zero_bos=True
        )
        tl_logits_zero, tl_activations_zero = tl_model.get_activations(input_text, zero_bos=True)

        # Test zero_bos=False
        nnsight_logits_no_zero, nnsight_activations_no_zero = nnsight_model.get_activations(
            input_text, zero_bos=False
        )
        tl_logits_no_zero, tl_activations_no_zero = tl_model.get_activations(
            input_text, zero_bos=False
        )

        # Check that results are close between models for same zero_bos setting
        assert torch.allclose(nnsight_logits_zero, tl_logits_zero, atol=1e-3, rtol=1e-3)
        assert torch.allclose(nnsight_activations_zero, tl_activations_zero, atol=1e-3, rtol=1e-3)

        assert torch.allclose(nnsight_logits_no_zero, tl_logits_no_zero, atol=1e-3, rtol=1e-3)
        assert torch.allclose(
            nnsight_activations_no_zero, tl_activations_no_zero, atol=1e-3, rtol=1e-3
        )

        # When zero_bos=True, first position activations should be zero for both models
        if nnsight_activations_zero.size(1) > 0:  # Check if we have positions
            first_pos_nnsight = nnsight_activations_zero[:, 0, :]  # [layers, features]
            first_pos_tl = tl_activations_zero[:, 0, :]

            assert torch.allclose(first_pos_nnsight, torch.zeros_like(first_pos_nnsight), atol=1e-6)
            assert torch.allclose(first_pos_tl, torch.zeros_like(first_pos_tl), atol=1e-6)

    def test_get_activations_apply_activation_function_option(self, models, test_inputs):
        """Test apply_activation_function option for both models."""
        nnsight_model, tl_model = models
        input_text = test_inputs[0]  # Use first test input

        # Test apply_activation_function=True (default)
        nnsight_logits_act, nnsight_activations_act = nnsight_model.get_activations(
            input_text, apply_activation_function=True
        )
        tl_logits_act, tl_activations_act = tl_model.get_activations(
            input_text, apply_activation_function=True
        )

        # Test apply_activation_function=False
        nnsight_logits_no_act, nnsight_activations_no_act = nnsight_model.get_activations(
            input_text, apply_activation_function=False
        )
        tl_logits_no_act, tl_activations_no_act = tl_model.get_activations(
            input_text, apply_activation_function=False
        )

        # Logits should be the same regardless of activation function application
        assert torch.allclose(nnsight_logits_act, nnsight_logits_no_act, atol=1e-6)
        assert torch.allclose(tl_logits_act, tl_logits_no_act, atol=1e-6)

        # Results should be close between models for same setting
        assert torch.allclose(nnsight_logits_act, tl_logits_act, atol=1e-3, rtol=1e-3)
        assert torch.allclose(nnsight_activations_act, tl_activations_act, atol=1e-3, rtol=1e-3)

        assert torch.allclose(nnsight_logits_no_act, tl_logits_no_act, atol=1e-3, rtol=1e-3)
        assert torch.allclose(
            nnsight_activations_no_act, tl_activations_no_act, atol=1e-3, rtol=1e-3
        )

        # Pre-activation and post-activation should be different (unless using linear activation)
        activation_diff_nnsight = torch.abs(
            nnsight_activations_act - nnsight_activations_no_act
        ).max()
        activation_diff_tl = torch.abs(tl_activations_act - tl_activations_no_act).max()

        # Should have some difference due to activation function (unless all values are negative)
        assert activation_diff_nnsight >= 0  # At minimum, should be non-negative
        assert activation_diff_tl >= 0

    def test_get_activations_tensor_input(self, models):
        """Test get_activations with tensor input instead of string."""
        nnsight_model, tl_model = models

        # Create a simple token tensor (assuming typical vocab size and tokenizer)
        # Using some common token IDs that should work with Gemma tokenizer
        token_ids = torch.tensor([1, 22557, 2134], dtype=torch.long)  # Example token IDs
        if torch.cuda.is_available():
            token_ids = token_ids.cuda()

        # Get activations from both models using tensor input
        try:
            nnsight_logits, nnsight_activations = nnsight_model.get_activations(token_ids)
            tl_logits, tl_activations = tl_model.get_activations(token_ids)

            # Check shapes are consistent
            assert nnsight_logits.shape == tl_logits.shape
            assert nnsight_activations.shape == tl_activations.shape

            # Check values are close
            assert torch.allclose(nnsight_logits, tl_logits, atol=1e-3, rtol=1e-3)
            assert torch.allclose(nnsight_activations, tl_activations, atol=1e-3, rtol=1e-3)

        except Exception as e:
            print(f"Tensor input test failed, possibly due to tokenizer incompatibility: {e}")

    def test_get_activations_batch_consistency(self, models, test_inputs):
        """Test that individual inputs produce same results as when processed separately."""
        nnsight_model, tl_model = models

        # Test each input individually to ensure consistency
        for i, input_text in enumerate(
            test_inputs[:2]
        ):  # Test first 2 inputs to keep test time reasonable
            # Run the same input multiple times
            results_nnsight = []
            results_tl = []

            for _ in range(2):  # Run twice to check consistency
                nnsight_result = nnsight_model.get_activations(input_text)
                tl_result = tl_model.get_activations(input_text)
                results_nnsight.append(nnsight_result)
                results_tl.append(tl_result)

            # Check that same input gives same result (deterministic)
            assert torch.allclose(results_nnsight[0][0], results_nnsight[1][0], atol=1e-6), (
                f"nnsight model not deterministic for input {i}"
            )
            assert torch.allclose(results_nnsight[0][1], results_nnsight[1][1], atol=1e-6), (
                f"nnsight model activations not deterministic for input {i}"
            )

            assert torch.allclose(results_tl[0][0], results_tl[1][0], atol=1e-6), (
                f"TL model not deterministic for input {i}"
            )
            assert torch.allclose(results_tl[0][1], results_tl[1][1], atol=1e-6), (
                f"TL model activations not deterministic for input {i}"
            )


# Additional test functions for edge cases and comprehensive coverage
class TestGetActivationsEdgeCases:
    """Additional tests for edge cases and specific scenarios."""

    @pytest.fixture(scope="class")
    def small_models(self):
        """Load smaller/faster models for edge case testing."""
        # Using the same models as main tests but with GPU
        model_name = "google/gemma-2-2b"
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        nnsight_model = ReplacementModel.from_pretrained(
            model_name, "gemma", device=device, dtype=torch.bfloat16, backend="nnsight"
        )
        tl_model = ReplacementModel.from_pretrained(
            model_name, "gemma", device=device, dtype=torch.bfloat16
        )

        return nnsight_model, tl_model

    def test_single_token_input(self, small_models):
        """Test with single token input."""
        nnsight_model, tl_model = small_models

        # Single character that should tokenize to one token
        input_text = "a"

        nnsight_logits, nnsight_activations = nnsight_model.get_activations(input_text)
        tl_logits, tl_activations = tl_model.get_activations(input_text)

        # Check consistency
        assert nnsight_logits.shape == tl_logits.shape
        assert nnsight_activations.shape == tl_activations.shape
        assert torch.allclose(nnsight_logits, tl_logits, atol=1e-3, rtol=1e-3)
        assert torch.allclose(nnsight_activations, tl_activations, atol=1e-3, rtol=1e-3)

    def test_special_characters_input(self, small_models):
        """Test with special characters and unicode."""
        nnsight_model, tl_model = small_models

        special_inputs = [
            "Hello! üòä",
            "Test with numbers: 123",
            "Symbols: @#$%^&*()",
            "Unicode: caf√© na√Øve r√©sum√©",
        ]

        for input_text in special_inputs:
            try:
                nnsight_logits, nnsight_activations = nnsight_model.get_activations(input_text)
                tl_logits, tl_activations = tl_model.get_activations(input_text)

                # Check consistency
                assert nnsight_logits.shape == tl_logits.shape
                assert nnsight_activations.shape == tl_activations.shape
                assert torch.allclose(nnsight_logits, tl_logits, atol=1e-3, rtol=1e-3)
                assert torch.allclose(nnsight_activations, tl_activations, atol=1e-3, rtol=1e-3)

            except Exception as e:
                # Some special characters might cause issues, that's OK
                print(f"Warning: Special character test failed for '{input_text}': {e}")
                continue


# Standalone execution functionality
def load_models():
    """Load both models for standalone testing."""
    model_name = "google/gemma-2-2b"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load nnsight model
    nnsight_model = ReplacementModel.from_pretrained(
        model_name, "gemma", device=device, dtype=torch.bfloat16, backend="nnsight"
    )

    # Load transformer_lens model
    tl_model = ReplacementModel.from_pretrained(
        model_name, "gemma", device=device, dtype=torch.bfloat16
    )

    return nnsight_model, tl_model


def main():
    """Main function for standalone execution."""
    # Load models
    models = load_models()

    # Test inputs
    test_inputs = [
        "Hello world",
        "The quick brown fox jumps over the lazy dog",
        "This is a test sentence for the activation comparison.",
        "Apple is a company that makes iPhones and computers",
    ]

    # Create test instances
    main_test = TestGetActivations()
    edge_test = TestGetActivationsEdgeCases()

    # Run all main tests
    main_test.test_get_activations_shape_consistency(models, test_inputs)
    main_test.test_get_activations_values_close(models, test_inputs)
    main_test.test_get_activations_sparse_option(models, test_inputs)
    main_test.test_get_activations_zero_bos_option(models, test_inputs)
    main_test.test_get_activations_apply_activation_function_option(models, test_inputs)
    main_test.test_get_activations_tensor_input(models)
    main_test.test_get_activations_batch_consistency(models, test_inputs)

    # Run edge case tests
    edge_test.test_single_token_input(models)
    edge_test.test_special_characters_input(models)


if __name__ == "__main__":
    main()
